{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "private_outputs": true,
      "authorship_tag": "ABX9TyMGRV7iA2JwEooRgHF5Cgkt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbhaidasna/ProductHealthSentimentAnalyst_HybridRAG/blob/main/ProductHealthSentimentAnalyst_HybridRAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ“˜ Product Health & Sentiment Analyst  \n",
        "### Hybrid RAG Decision Primitive for Product Prioritization (v1.0)\n",
        "\n",
        "---\n",
        "\n",
        "## Why This Notebook Exists (PM First Principles)\n",
        "\n",
        "One of the most critical and difficult skills of a Product Manager is **prioritization**.\n",
        "\n",
        "In practice, prioritization is not a single signal or score. It requires **triangulating multiple dimensions**, including:\n",
        "\n",
        "- **Customer Value**  \n",
        "  Who is impacted? How severely? Are high-value customers affected?\n",
        "- **Business Viability**  \n",
        "  Revenue risk, contractual exposure, brand trust, and strategic alignment\n",
        "- **Technical Feasibility**  \n",
        "  Severity, ownership, technical debt, and effort to resolve\n",
        "\n",
        "PMs then weight these signals differently depending on the companyâ€™s current strategic focus.\n",
        "\n",
        "The challenge is that these signals rarely live in one place. They are fragmented across:\n",
        "- long-form documentation,\n",
        "- historical tracking systems,\n",
        "- ownership and escalation graphs,\n",
        "- and real-time customer sentiment.\n",
        "\n",
        "This notebook explores how **agentic AI systems can support PM judgment** by reducing the cost of context aggregation, without replacing decision-making.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Notebook Is and Is Not\n",
        "\n",
        "### What this notebook demonstrates\n",
        "\n",
        "This notebook implements a **decision primitive** for PM prioritization.\n",
        "\n",
        "Specifically, it answers:\n",
        "\n",
        "> **â€œShould this issue be prioritized right now, and why?â€**\n",
        "\n",
        "To do that, it:\n",
        "- retrieves **semantic context** from documentation,\n",
        "- retrieves **structural truth** from historical graphs,\n",
        "- retrieves **fresh signals** from real-time systems,\n",
        "- and synthesizes them into a **traceable, confidence-aware recommendation**.\n",
        "\n",
        "This is a foundational building block for more advanced PM systems.\n",
        "\n",
        "---\n",
        "\n",
        "### What this notebook does not attempt to do\n",
        "\n",
        "This notebook does not attempt to:\n",
        "- generate a full roadmap,\n",
        "- rank a portfolio of features,\n",
        "- or make autonomous product decisions.\n",
        "\n",
        "At a higher level, PMs ultimately want to ask:\n",
        "\n",
        "> **â€œGiven all available signals, what are the top 10 issues or features I should evaluate for my roadmap right now?â€**\n",
        "\n",
        "That portfolio-level problem requires many such decision primitives, plus additional layers for:\n",
        "- strategic weighting,\n",
        "- capacity constraints,\n",
        "- sequencing,\n",
        "- and organizational context.\n",
        "\n",
        "This notebook intentionally focuses on **getting one critical decision right**, end to end.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Use-Case (Decision Primitive)\n",
        "\n",
        "### Persona: Eliza, Product Manager\n",
        "\n",
        "**Elizaâ€™s problem**  \n",
        "Eliza needs to justify her prioritization decisions to leadership.  \n",
        "She must quickly answer questions such as:\n",
        "\n",
        "- Is this issue impacting high-value customers?\n",
        "- Is it still happening right now?\n",
        "- How severe is it compared to other issues?\n",
        "- Can I defend this decision with evidence?\n",
        "\n",
        "Her data is fragmented across:\n",
        "- internal PRDs and design documents,\n",
        "- bug tracking systems,\n",
        "- ownership and escalation graphs,\n",
        "- and public or community forums.\n",
        "\n",
        "### User Story\n",
        "\n",
        "> As a Product Manager, I want to query the impact of a feature or bug across historical and real-time systems so I can confidently prioritize work on my roadmap.\n",
        "\n",
        "---\n",
        "\n",
        "## Why Hybrid RAG Is the Right Reference Architecture\n",
        "\n",
        "No single retrieval method is sufficient for PM decision-making.\n",
        "\n",
        "### 1. Semantic RAG (Vector Search)\n",
        "\n",
        "**What it is good at**\n",
        "- Long-form narrative context\n",
        "- PRDs, design rationale, and historical discussions\n",
        "\n",
        "**What it lacks**\n",
        "- Authority\n",
        "- Explicit relationships\n",
        "- Freshness guarantees\n",
        "\n",
        "---\n",
        "\n",
        "### 2. Graph RAG, Structural (Neo4j)\n",
        "\n",
        "**What it is good at**\n",
        "- Historical truth\n",
        "- Ownership, escalation paths, and severity\n",
        "- Multi-hop reasoning across entities\n",
        "\n",
        "**What it lacks**\n",
        "- Real-time relevance\n",
        "- Qualitative nuance\n",
        "\n",
        "---\n",
        "\n",
        "### 3. Graph RAG, Real-Time (Memgraph)\n",
        "\n",
        "**What it is good at**\n",
        "- Fresh signals\n",
        "- Low-latency sentiment and activity\n",
        "\n",
        "**What it lacks**\n",
        "- Completeness\n",
        "- Authority\n",
        "- Noise resistance\n",
        "\n",
        "---\n",
        "\n",
        "### Why PM decisions require all three\n",
        "\n",
        "PM reasoning mirrors this structure:\n",
        "\n",
        "> **Historical truth + Structural authority + Current reality**\n",
        "\n",
        "This notebook uses **Hybrid RAG** to intentionally reflect how PMs already think, but at machine speed, with explicit confidence and traceability.\n",
        "\n",
        "---\n",
        "\n",
        "## High-Level Architecture Overview\n",
        "\n",
        "The system executes three parallel retrieval paths, normalizes their outputs, and delegates synthesis to a single analytical authority.\n",
        "\n",
        "| Retrieval Type | Data Source | Purpose |\n",
        "|---------------|------------|---------|\n",
        "| Semantic RAG | FAISS | Long-form contextual understanding |\n",
        "| GraphRAG, Structural | Neo4j Aura | Bugs, ownership, escalation paths |\n",
        "| GraphRAG, Real-Time | Memgraph Cloud | Fresh community sentiment and activity |\n",
        "\n",
        "Each signal is surfaced with **freshness** and **confidence** metadata to prevent over-trust.\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Structure (Implementation Stages)\n",
        "\n",
        "This notebook is organized to reflect a **real system build**, not a demo.\n",
        "\n",
        "### Stage 1: Environment, Dependencies, and Observability\n",
        "- Dependency installation\n",
        "- Secure secret loading\n",
        "- LangSmith tracing and observability\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 2: Traditional RAG, Semantic Knowledge (FAISS)\n",
        "- Document ingestion and chunking\n",
        "- Embedding generation\n",
        "- FAISS index creation\n",
        "- Semantic retriever construction\n",
        "\n",
        "**Purpose:** Provide foundational narrative context.\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 3: Graph Substrates and Intelligence Primitives\n",
        "\n",
        "#### Stage 3.1: Neo4j Structural Knowledge\n",
        "- Graph schema definition\n",
        "- Historical bug, customer, and ownership data\n",
        "- Durable, authoritative relationships\n",
        "\n",
        "**Purpose:** Answer multi-hop structural questions.\n",
        "\n",
        "---\n",
        "\n",
        "#### Stage 3.2: Memgraph Real-Time Data Ingestion (Production-Safe)\n",
        "- Timestamped forum posts\n",
        "- Deterministic sentiment relationships\n",
        "- Time-window freshness guarantees\n",
        "\n",
        "**Purpose:** Capture current user impact.\n",
        "\n",
        "---\n",
        "\n",
        "#### Stage 3.3: Intelligence Primitives (Claude)\n",
        "- Explicit role separation  \n",
        "  - Translator (Cypher compiler)  \n",
        "  - Synthesizer (PM reasoning)\n",
        "- Model capability guards\n",
        "\n",
        "**Purpose:** Prevent silent model misuse.\n",
        "\n",
        "---\n",
        "\n",
        "#### Stage 3.4: LLM-as-a-Coder\n",
        "- Natural language to Cypher translation\n",
        "- Schema-aware constraints\n",
        "- Read-only guarantees\n",
        "\n",
        "**Purpose:** Enable flexible graph interrogation without hand-written queries.\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 4: Orchestration\n",
        "- Parallel retriever execution\n",
        "- Context normalization\n",
        "- Confidence and freshness annotation\n",
        "- Final synthesis with traceability\n",
        "\n",
        "---\n",
        "\n",
        "### Stage 5: Evaluations\n",
        "- Retriever-level inspection\n",
        "- Hybrid RAG ablation testing\n",
        "- Decision quality and confidence review\n",
        "\n",
        "**Purpose:** Validate whether architecture choices materially improve PM decisions.\n",
        "\n",
        "---\n",
        "\n",
        "## What This Notebook Ultimately Demonstrates\n",
        "\n",
        "- Why **architecture choices matter** for PM decision quality\n",
        "- How to detect stale, missing, or conflicting signals\n",
        "- When to trust which retrieval method and when not to\n",
        "- How agentic AI can **augment judgment**, not replace it\n",
        "\n",
        "This is not a toy agent.  \n",
        "It is a **reference implementation for decision-centric Agentic AI systems**.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "m_1NBLWnpSAa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bQ5blkCUnp62"
      },
      "outputs": [],
      "source": [
        "# =============================================================\n",
        "# Stage 1: ENVIRONMENT, DEPENDENCIES & OBSERVABILITY\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Create a stable, deterministic runtime\n",
        "# - Install all dependencies ONCE\n",
        "# - Load secrets securely\n",
        "# - Enable LangSmith tracing for debugging\n",
        "#\n",
        "# IMPORTANT RULES:\n",
        "# - No business logic\n",
        "# - No LLM calls\n",
        "# - No retrieval\n",
        "#\n",
        "# If something breaks here, the system should not proceed.\n",
        "# =============================================================\n",
        "\n",
        "# ---- Install dependencies (ONE TIME ONLY) ----\n",
        "!pip install -qU \\\n",
        "  langchain \\\n",
        "  langchain-core \\\n",
        "  langchain-community \\\n",
        "  langchain-experimental \\\n",
        "  langchain-nvidia-ai-endpoints \\\n",
        "  langchain-neo4j \\\n",
        "  neo4j \\\n",
        "  pymgclient \\\n",
        "  faiss-cpu \\\n",
        "  unstructured \\\n",
        "  python-docx\n",
        "\n",
        "!pip install -qU langchain-anthropic\n",
        "\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "# ---- NVIDIA API (LLMs + Embeddings) ----\n",
        "os.environ[\"NVIDIA_API_KEY\"] = userdata.get(\"NVIDIA_API_KEY\")\n",
        "\n",
        "#Setting Claude API Key - This will be used for Cypher Translator & Response Synthesizer\n",
        "os.environ[\"ANTHROPIC_API_KEY\"] = userdata.get(\"ANTHROPIC_API_KEY\")\n",
        "\n",
        "# ---- LangSmith Observability ----\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"LANGCHAIN_API_KEY\")\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"ProductHealthSentimentAnalyst-HybridRAG\"\n",
        "\n",
        "print(\"Environment initialized. Dependencies installed. Tracing enabled.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 2: TRADITIONAL RAG - SEMANTIC KNOWLEDGE (FAISS)\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Load unstructured documentation\n",
        "# - Chunk and embed text\n",
        "# - Build an in-memory FAISS index\n",
        "#\n",
        "# THIS IS NOT INTELLIGENCE.\n",
        "# This is static semantic memory.\n",
        "# =============================================================\n",
        "\n",
        "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.document_loaders import UnstructuredWordDocumentLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "# ---- Configuration ----\n",
        "FILE_PATH = \"BMT_Internal_Documentation.docx\"\n",
        "EMBED_MODEL = \"nvidia/nv-embedqa-e5-v5\"\n",
        "\n",
        "# ---- Embeddings ----\n",
        "embeddings = NVIDIAEmbeddings(model=EMBED_MODEL)\n",
        "\n",
        "# ---- Load document ----\n",
        "loader = UnstructuredWordDocumentLoader(FILE_PATH, mode=\"single\")\n",
        "documents = loader.load()\n",
        "\n",
        "# ---- Chunking ----\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=80,\n",
        ")\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "# ---- FAISS Vector Store ----\n",
        "faiss_db = FAISS.from_documents(chunks, embeddings)\n",
        "faiss_retriever = faiss_db.as_retriever(search_kwargs={\"k\": 5})\n",
        "\n",
        "print(f\"FAISS ready. Total chunks indexed: {len(chunks)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "fY_0cQ9737RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 3: GRAPH SUBSTRATES + INTELLIGENCE PRIMITIVES\n",
        "# Stage 3.1: NEO4J STRUCTURAL KNOWLEDGE\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Store durable, relational product data\n",
        "# - Bugs, customers, severity, escalation, ownership\n",
        "#\n",
        "# NO LLM LOGIC HERE.\n",
        "# =============================================================\n",
        "\n",
        "from neo4j import GraphDatabase\n",
        "from langchain_neo4j import Neo4jGraph\n",
        "from google.colab import userdata\n",
        "\n",
        "NEO4J_URI = userdata.get(\"NEO4J_URI\")\n",
        "NEO4J_PASSWORD = userdata.get(\"NEO4J_PASSWORD\")\n",
        "\n",
        "HISTORICAL_DATA_CYPHER = [\n",
        "    \"MATCH (n) DETACH DELETE n\",\n",
        "\n",
        "    \"MERGE (f:Feature {name: 'Bio-Metric Tracker'})\",\n",
        "\n",
        "    \"MERGE (e:Employee {name: 'Mark Olsen', title: 'VP Eng'})\",\n",
        "    \"MATCH (f:Feature), (e:Employee) MERGE (f)-[:REVIEWED_BY]->(e)\",\n",
        "\n",
        "    \"MERGE (b:Bug {id: 'BMT-CRIT-042', severity: 'Critical', initial_severity: 'Medium'})\",\n",
        "    \"MATCH (f:Feature), (b:Bug) MERGE (f)-[:HAS_BUG]->(b)\",\n",
        "\n",
        "    \"MERGE (c:Customer {id: 'CUST-001', value: 'HighValue'})\",\n",
        "    \"MATCH (c:Customer), (b:Bug) MERGE (c)-[:REPORTED_BY]->(b)\",\n",
        "\n",
        "    \"MERGE (t:Team {name: 'Team Alpha'})\",\n",
        "    \"MATCH (b:Bug), (t:Team) MERGE (b)-[:ESCALATED_TO]->(t)\",\n",
        "]\n",
        "\n",
        "def load_neo4j_data():\n",
        "    driver = GraphDatabase.driver(\n",
        "        NEO4J_URI, auth=(\"neo4j\", NEO4J_PASSWORD)\n",
        "    )\n",
        "    with driver.session() as session:\n",
        "        for stmt in HISTORICAL_DATA_CYPHER:\n",
        "            session.run(stmt)\n",
        "    driver.close()\n",
        "\n",
        "load_neo4j_data()\n",
        "\n",
        "neo4j_graph = Neo4jGraph(\n",
        "    url=NEO4J_URI,\n",
        "    username=\"neo4j\",\n",
        "    password=NEO4J_PASSWORD,\n",
        ")\n",
        "\n",
        "print(\"Neo4j structural data loaded.\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "swq8pQ8oCdeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# =============================================================\n",
        "# Stage 3.2: MEMGRAPH REAL-TIME DATA INGESTION (PRODUCTION-SAFE)\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Insert forum posts with timestamps\n",
        "# - Enforce exactly ONE sentiment per post\n",
        "# - Prevent sentiment accumulation / row explosion\n",
        "#\n",
        "# DESIGN PRINCIPLES:\n",
        "# - Idempotent (safe to re-run)\n",
        "# - Deterministic\n",
        "# - Easy to debug\n",
        "# =============================================================\n",
        "\n",
        "\n",
        "import time\n",
        "import mgclient\n",
        "from google.colab import userdata\n",
        "\n",
        "# ---- Memgraph API ----\n",
        "MEMGRAPH_HOST = userdata.get(\"MEMGRAPH_HOST\")\n",
        "MEMGRAPH_PORT = int(userdata.get(\"MEMGRAPH_PORT\"))\n",
        "MEMGRAPH_USER = userdata.get(\"MEMGRAPH_USER\")\n",
        "MEMGRAPH_PASSWORD = userdata.get(\"MEMGRAPH_PASSWORD\")\n",
        "\n",
        "NOW_US = int(time.time() * 1_000_000)\n",
        "\n",
        "conn = mgclient.connect(\n",
        "    host=MEMGRAPH_HOST,\n",
        "    port=MEMGRAPH_PORT,\n",
        "    username=MEMGRAPH_USER,\n",
        "    password=MEMGRAPH_PASSWORD,\n",
        "    sslmode=mgclient.MG_SSLMODE_REQUIRE,\n",
        ")\n",
        "cursor = conn.cursor()\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# HARD RESET SENTIMENT RELATIONSHIPS (BOTH DIRECTIONS)\n",
        "# -------------------------------------------------------------\n",
        "cursor.execute(\"\"\"\n",
        "MATCH (:ForumPost)-[r:LINKED_TO]-(:Sentiment)\n",
        "DELETE r\n",
        "\"\"\")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# INGEST POSTS WITH EXACTLY ONE SENTIMENT\n",
        "# -------------------------------------------------------------\n",
        "posts = [\n",
        "    {\n",
        "        \"text\": \"The BMT just failed again!\",\n",
        "        \"sentiment\": \"Negative\",\n",
        "        \"timestamp\": NOW_US - (5 * 60 * 1_000_000),\n",
        "    },\n",
        "    {\n",
        "        \"text\": \"Sync issue fixed!\",\n",
        "        \"sentiment\": \"Positive\",\n",
        "        \"timestamp\": NOW_US - (10 * 60 * 1_000_000),\n",
        "    },\n",
        "]\n",
        "\n",
        "for post in posts:\n",
        "    cursor.execute(\n",
        "        \"\"\"\n",
        "        MERGE (fp:ForumPost {text: $text})\n",
        "        SET fp.timestamp = $timestamp\n",
        "        WITH fp\n",
        "        MERGE (s:Sentiment {type: $sentiment})\n",
        "        MERGE (fp)-[:LINKED_TO]->(s)\n",
        "        \"\"\",\n",
        "        {\n",
        "            \"text\": post[\"text\"],\n",
        "            \"timestamp\": post[\"timestamp\"],\n",
        "            \"sentiment\": post[\"sentiment\"],\n",
        "        },\n",
        "    )\n",
        "\n",
        "conn.commit()\n",
        "conn.close()\n",
        "\n",
        "print(\"Memgraph ingestion complete (sentiment relationships normalized).\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ks54uPEl2X3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# STAGE 3.3: INTELLIGENCE PRIMITIVES (CLAUDE)\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Define LLM roles with strict responsibility boundaries\n",
        "# - Use only models VERIFIED to be enabled for this account\n",
        "#\n",
        "# ROLES:\n",
        "# - Cypher LLM     â†’ Translator / Compiler\n",
        "# - Synthesis LLM  â†’ Analyst / Decision Maker\n",
        "# =============================================================\n",
        "\n",
        "from langchain_anthropic import ChatAnthropic\n",
        "import os\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Cypher LLM (Translator / Compiler)\n",
        "#\n",
        "# Requirements:\n",
        "# - Deterministic\n",
        "# - Precise\n",
        "# - Schema-faithful\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "cypher_llm = ChatAnthropic(\n",
        "    model=\"claude-3-haiku-20240307\",\n",
        "    temperature=0.0,\n",
        "    max_tokens=512,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Synthesis LLM (Analyst / PM Reasoner)\n",
        "#\n",
        "# Requirements:\n",
        "# - Multi-source reasoning\n",
        "# - Long context\n",
        "# - Clear, grounded explanations\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "synthesis_llm = ChatAnthropic(\n",
        "    model=\"claude-3-7-sonnet-20250219\",\n",
        "    temperature=0.2,\n",
        "    max_tokens=1200,\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "# Sanity checks â€” BOTH MUST PASS\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "print(\"Cypher LLM check:\")\n",
        "print(cypher_llm.invoke(\"Return exactly the word: OK\"))\n",
        "\n",
        "print(\"\\nSynthesis LLM check:\")\n",
        "print(synthesis_llm.invoke(\"Respond with exactly: CHAT OK\"))\n",
        "\n"
      ],
      "metadata": {
        "id": "uKuMQyH_5289"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 3.4: LLM-AS-A-CODER\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Convert natural language â†’ Cypher\n",
        "# - Act as a deterministic Cypher compiler\n",
        "# - NEVER mutate the graph\n",
        "# =============================================================\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "GRAPH_SCHEMA = \"\"\"\n",
        "You are an expert Cypher query generator.\n",
        "\n",
        "GRAPH SCHEMA:\n",
        "\n",
        "Nodes:\n",
        "- Bug:\n",
        "  - id (string, unique)\n",
        "  - severity (string)\n",
        "  - initial_severity (string)\n",
        "\n",
        "- Customer:\n",
        "  - id (string)\n",
        "  - value (string, e.g. 'HighValue')\n",
        "\n",
        "- Feature:\n",
        "  - name (string)\n",
        "\n",
        "- Employee:\n",
        "  - name (string)\n",
        "  - title (string)\n",
        "\n",
        "- Team:\n",
        "  - name (string)\n",
        "\n",
        "Relationships:\n",
        "- (:Feature)-[:HAS_BUG]->(:Bug)\n",
        "- (:Customer)-[:REPORTED_BY]->(:Bug)\n",
        "- (:Bug)-[:ESCALATED_TO]->(:Team)\n",
        "- (:Feature)-[:REVIEWED_BY]->(:Employee)\n",
        "\n",
        "IMPORTANT RULES:\n",
        "- Always match Bug nodes using `id`, not `name`\n",
        "- Respect relationship directions\n",
        "- Use OPTIONAL MATCH for non-mandatory relationships\n",
        "- Generate READ-ONLY queries only\n",
        "- Do NOT use CREATE, MERGE, DELETE, or SET\n",
        "- Return all relevant connected nodes\n",
        "\n",
        "Return ONLY valid Cypher. No explanations.\n",
        "\"\"\"\n",
        "\n",
        "cypher_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", GRAPH_SCHEMA),\n",
        "    (\"human\", \"Question: {question}\")\n",
        "])\n",
        "\n",
        "def generate_cypher_query(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Deterministically converts a natural language question\n",
        "    into a READ-ONLY Cypher query.\n",
        "    \"\"\"\n",
        "    result = (cypher_prompt | cypher_llm).invoke({\"question\": question})\n",
        "    return result.content.strip()\n",
        "\n",
        "print(\"LLM-as-a-Coder ready and schema-aligned.\")\n"
      ],
      "metadata": {
        "id": "OwVgse408Zh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# STAGE 3.5 (HARDENED): CYPHER CONTRACT + VALIDATION\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Enforce deterministic Cypher structure\n",
        "# - Catch invalid query shapes BEFORE execution\n",
        "# - Convert DB errors into explicit system signals\n",
        "\n",
        "\"\"\" Here's how the flow is defined\n",
        "[ Stage 3.4: LLM-as-a-Coder ]\n",
        "        â†“\n",
        "( generates candidate Cypher )\n",
        "        â†“\n",
        "[ Stage 3.5: Cypher Contract + Validator ]   â† NEW\n",
        "        â†“\n",
        "( approved Cypher only )\n",
        "        â†“\n",
        "[ Stage 4 (as part of orchestration): Neo4j Execution ]\"\"\"\n",
        "\n",
        "# =============================================================\n",
        "\n",
        "import re\n",
        "\n",
        "def validate_cypher(cypher: str):\n",
        "    \"\"\"\n",
        "    Deterministic Cypher validator.\n",
        "    This is NOT about correctness of results,\n",
        "    but correctness of query SHAPE.\n",
        "    \"\"\"\n",
        "\n",
        "    # Rule 1: Bug must be matched by id\n",
        "    if \"Bug\" in cypher and \"id:\" not in cypher:\n",
        "        raise ValueError(\n",
        "            \"Cypher contract violation: Bug nodes must be matched using `id`\"\n",
        "        )\n",
        "\n",
        "    # Rule 2: WHERE after OPTIONAL MATCH requires WITH\n",
        "    if \"OPTIONAL MATCH\" in cypher and \"WHERE\" in cypher:\n",
        "        optional_index = cypher.find(\"OPTIONAL MATCH\")\n",
        "        where_index = cypher.find(\"WHERE\")\n",
        "        with_index = cypher.find(\"WITH\")\n",
        "\n",
        "        if where_index > optional_index and (with_index == -1 or with_index > where_index):\n",
        "            raise ValueError(\n",
        "                \"Cypher contract violation: WHERE after OPTIONAL MATCH requires WITH\"\n",
        "            )\n",
        "\n",
        "    # Rule 3: Disallow destructive operations\n",
        "    forbidden = [\"DELETE\", \"DETACH\", \"SET\", \"CREATE\", \"MERGE\"]\n",
        "    for word in forbidden:\n",
        "        if re.search(rf\"\\b{word}\\b\", cypher, re.IGNORECASE):\n",
        "            raise ValueError(\n",
        "                f\"Cypher contract violation: `{word}` operations are not allowed\"\n",
        "            )\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "U8CsExuuB-IY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 4: ORCHESTRATION\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Run all retrievers in parallel\n",
        "# - Inspect and debug each retrieverâ€™s output\n",
        "# - Normalize heterogeneous signals\n",
        "# - Invoke synthesis LLM\n",
        "#\n",
        "# NO CONFIGURATION. NO MODEL CHOICES.\n",
        "# =============================================================\n",
        "\n",
        "from langchain_core.runnables import (\n",
        "    RunnableLambda,\n",
        "    RunnableParallel,\n",
        "    RunnablePassthrough,\n",
        ")\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from operator import itemgetter\n",
        "from typing import List\n",
        "import mgclient\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# DEBUG WRAPPER\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Print exactly what each retriever returns\n",
        "# - Preserve documents unchanged for downstream steps\n",
        "# =============================================================\n",
        "\n",
        "def debug_retriever_output(name: str, docs: List[Document]) -> List[Document]:\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(f\"RETRIEVER: {name}\")\n",
        "    print(f\"DOCUMENT COUNT: {len(docs)}\")\n",
        "\n",
        "    if not docs:\n",
        "        print(\"âš ï¸ WARNING: Retriever returned NO documents\")\n",
        "\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    for i, doc in enumerate(docs, 1):\n",
        "        print(f\"\\nDocument {i}\")\n",
        "        print(f\"Source: {doc.metadata.get('source')}\")\n",
        "        print(\"Content:\")\n",
        "        print(doc.page_content)\n",
        "\n",
        "    print(\"=\" * 80 + \"\\n\")\n",
        "    return docs\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# NEO4J RETRIEVER (STRUCTURAL) â€” HARDENED\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Execute ONLY validated Cypher\n",
        "# - Enforce determinism outside the LLM\n",
        "# - Fail safely with explicit confidence downgrade\n",
        "# =============================================================\n",
        "\n",
        "def neo4j_retriever(question: str) -> List[Document]:\n",
        "    try:\n",
        "        # 1. LLM proposes Cypher (Stage 3.4)\n",
        "        cypher = generate_cypher_query(question)\n",
        "\n",
        "        # 2. Deterministic enforcement (Stage 3.4a)\n",
        "        validate_cypher(cypher)\n",
        "\n",
        "        # 3. Execute only validated Cypher\n",
        "        result = neo4j_graph.query(cypher)\n",
        "\n",
        "        confidence = \"high\" if result else \"medium\"\n",
        "\n",
        "        docs = [\n",
        "            Document(\n",
        "                page_content=(\n",
        "                    \"NEO4J STRUCTURAL DATA\\n\"\n",
        "                    f\"Cypher:\\n{cypher}\\n\\n\"\n",
        "                    f\"Result:\\n{result}\"\n",
        "                ),\n",
        "                metadata={\n",
        "                    \"source\": \"Neo4j\",\n",
        "                    \"freshness\": \"unknown\",\n",
        "                    \"confidence\": confidence,\n",
        "                },\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    except Exception as e:\n",
        "        # 4. Fail explicitly and safely\n",
        "        docs = [\n",
        "            Document(\n",
        "                page_content=(\n",
        "                    \"NEO4J STRUCTURAL DATA\\n\"\n",
        "                    \"Query failed validation or execution.\\n\"\n",
        "                    f\"Error: {str(e)}\"\n",
        "                ),\n",
        "                metadata={\n",
        "                    \"source\": \"Neo4j\",\n",
        "                    \"freshness\": \"unknown\",\n",
        "                    \"confidence\": \"low\",\n",
        "                    \"error\": \"cypher_validation_failure\",\n",
        "                },\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    return debug_retriever_output(\"Neo4j (Structural)\", docs)\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# MEMGRAPH RETRIEVER (REAL-TIME SENTIMENT) - Checking for last 1 day of forum posts\n",
        "# =============================================================\n",
        "\n",
        "def memgraph_retriever(_: str) -> List[Document]:\n",
        "    ONE_DAY_US = 24 * 60 * 60 * 1_000_000\n",
        "\n",
        "    query = (\n",
        "        \"MATCH (fp:ForumPost) \"\n",
        "        f\"WHERE fp.timestamp > (timestamp() - {ONE_DAY_US}) \"\n",
        "        \"OPTIONAL MATCH (fp)-[:LINKED_TO]->(s:Sentiment) \"\n",
        "        \"RETURN fp.text AS post, collect(DISTINCT s.type) AS sentiments, fp.timestamp AS ts\"\n",
        "    )\n",
        "\n",
        "    conn = mgclient.connect(\n",
        "        host=MEMGRAPH_HOST,\n",
        "        port=MEMGRAPH_PORT,\n",
        "        username=MEMGRAPH_USER,\n",
        "        password=MEMGRAPH_PASSWORD,\n",
        "        sslmode=mgclient.MG_SSLMODE_REQUIRE,\n",
        "    )\n",
        "    cursor = conn.cursor()\n",
        "    cursor.execute(query)\n",
        "    rows = cursor.fetchall()\n",
        "    conn.close()\n",
        "\n",
        "    if not rows:\n",
        "        return debug_retriever_output(\n",
        "            \"Memgraph (Real-Time)\",\n",
        "            [\n",
        "                Document(\n",
        "                    page_content=\"No forum posts in the last 1 day.\",\n",
        "                    metadata={\n",
        "                        \"source\": \"Memgraph\",\n",
        "                        \"freshness\": \"fresh_absent\",\n",
        "                        \"confidence\": \"low\",\n",
        "                    },\n",
        "                )\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    # Sort newest â†’ oldest\n",
        "    rows = sorted(rows, key=lambda r: r[2], reverse=True)\n",
        "\n",
        "    latest_post, latest_sentiments, latest_ts = rows[0]\n",
        "\n",
        "    confidence = \"high\" if len(rows) >= 2 else \"medium\"\n",
        "\n",
        "    docs = []\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 1ï¸âƒ£ CURRENT STATE (authoritative)\n",
        "    # -------------------------------------------------\n",
        "    docs.append(\n",
        "        Document(\n",
        "            page_content=(\n",
        "                f\"Current forum status: '{latest_post}' \"\n",
        "                f\"| sentiments={latest_sentiments}\"\n",
        "            ),\n",
        "            metadata={\n",
        "                \"source\": \"Memgraph\",\n",
        "                \"freshness\": \"fresh\",\n",
        "                \"confidence\": confidence,\n",
        "                \"role\": \"current_state\",\n",
        "            },\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # -------------------------------------------------\n",
        "    # 2ï¸âƒ£ RECENT HISTORY (context)\n",
        "    # -------------------------------------------------\n",
        "    history_lines = [\n",
        "        f\"- {post} | sentiments={sentiments}\"\n",
        "        for post, sentiments, _ in rows[1:]\n",
        "    ]\n",
        "\n",
        "    if history_lines:\n",
        "        docs.append(\n",
        "            Document(\n",
        "                page_content=\"Recent forum history:\\n\" + \"\\n\".join(history_lines),\n",
        "                metadata={\n",
        "                    \"source\": \"Memgraph\",\n",
        "                    \"freshness\": \"fresh\",\n",
        "                    \"confidence\": \"medium\",\n",
        "                    \"role\": \"history\",\n",
        "                },\n",
        "            )\n",
        "        )\n",
        "\n",
        "    return debug_retriever_output(\"Memgraph (Real-Time)\", docs)\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# FAISS RETRIEVER (SEMANTIC)\n",
        "# =============================================================\n",
        "\n",
        "def faiss_debug_retriever(question: str) -> List[Document]:\n",
        "    docs = faiss_retriever.invoke(question)\n",
        "\n",
        "    confidence = (\n",
        "        \"high\" if len(docs) >= 3\n",
        "        else \"medium\" if len(docs) > 0\n",
        "        else \"low\"\n",
        "    )\n",
        "\n",
        "    for doc in docs:\n",
        "        doc.metadata.update({\n",
        "            \"source\": \"FAISS\",\n",
        "            \"freshness\": \"unknown\",\n",
        "            \"confidence\": confidence,\n",
        "        })\n",
        "\n",
        "    return debug_retriever_output(\"FAISS (Semantic)\", docs)\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# PARALLEL RETRIEVAL (LangSmith Instrumented)\n",
        "#\n",
        "# PURPOSE:\n",
        "# - Run all retrievers concurrently\n",
        "# - Preserve signal separation\n",
        "# - Produce clean, traceable LangSmith spans\n",
        "#\n",
        "# Each retriever is explicitly named so that\n",
        "# LangSmith traces are readable and debuggable.\n",
        "# =============================================================\n",
        "\n",
        "parallel_retrieval = RunnableParallel(\n",
        "    {\n",
        "        \"semantic\": RunnableLambda(faiss_debug_retriever).with_config(\n",
        "            run_name=\"Retriever:FAISS:Semantic\"\n",
        "        ),\n",
        "        \"structural\": RunnableLambda(neo4j_retriever).with_config(\n",
        "            run_name=\"Retriever:Neo4j:Structural\"\n",
        "        ),\n",
        "        \"realtime\": RunnableLambda(memgraph_retriever).with_config(\n",
        "            run_name=\"Retriever:Memgraph:Realtime\"\n",
        "        ),\n",
        "    }\n",
        ").with_config(run_name=\"HybridRetrieval\")\n",
        "\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# CONTEXT NORMALIZATION\n",
        "# =============================================================\n",
        "\n",
        "def format_context(results: dict) -> str:\n",
        "    blocks = []\n",
        "\n",
        "    for key, docs in results.items():\n",
        "        for doc in docs:\n",
        "            meta = doc.metadata\n",
        "            header = (\n",
        "                f\"[{key.upper()}]\\n\"\n",
        "                f\"freshness: {meta.get('freshness')}\\n\"\n",
        "                f\"confidence: {meta.get('confidence')}\\n\"\n",
        "            )\n",
        "            blocks.append(header + \"\\n\" + doc.page_content)\n",
        "\n",
        "    return \"\\n\\n\".join(blocks)\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# FINAL PROMPT\n",
        "# =============================================================\n",
        "\n",
        "FINAL_PROMPT = ChatPromptTemplate.from_template(\"\"\"\n",
        "You are a Product Health & Sentiment Analyst.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Answer with clear reasoning and source attribution.\n",
        "If a signal is stale, unknown, or low-confidence, explicitly state uncertainty.\n",
        "If real-time signals are mixed, explicitly describe the tradeoff and explain how it affects prioritization.\n",
        "\n",
        "At the end of your answer, include a section titled \"Decision Confidence\" with one of:\n",
        "High, Medium, Low, or Insufficient.\n",
        "Briefly justify the confidence level based on signal freshness, consistency, and coverage.\n",
        "\n",
        "\"\"\")\n",
        "\n",
        "\n",
        "# =============================================================\n",
        "# FINAL CHAIN\n",
        "# =============================================================\n",
        "\n",
        "final_chain = (\n",
        "    RunnablePassthrough.assign(\n",
        "        context=itemgetter(\"question\")\n",
        "        | parallel_retrieval\n",
        "        | format_context\n",
        "    )\n",
        "    | FINAL_PROMPT\n",
        "    | synthesis_llm\n",
        "    | StrOutputParser()\n",
        ").with_config(run_name=\"Agent:ProductHealthDecision\")\n",
        "\n",
        "final_chain = final_chain.with_config(\n",
        "    tags=[\"hybrid-rag\", \"pm-agent\", \"product-health\"],\n",
        ")\n",
        "\n",
        "# =============================================================\n",
        "# TEST\n",
        "# =============================================================\n",
        "\n",
        "print(final_chain.invoke({\n",
        "    \"question\": \"Should we prioritize BMT-CRIT-042 and why?\"\n",
        "}))\n",
        "\n"
      ],
      "metadata": {
        "id": "8EbIs2Nd8v2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EVALUATIONS â€” PRODUCT HEALTH & SENTIMENT ANALYST\n",
        "\n",
        "\"\"\"Goal of this section\n",
        "Validate whether the Hybrid RAG system meaningfully improves PM decision-making, not just retrieval accuracy.\n",
        "We focus on three PM questions that represent:\n",
        "- prioritization\n",
        "- customer impact\n",
        "- real-time status \"\"\"\n",
        "\n",
        "# =============================================================\n",
        "# Stage 5A: PM QUESTIONS (TOP 3)\n",
        "# These questions act as the stable benchmark.\n",
        "# Every architectural or model change should be tested against these.\n",
        "# =============================================================\n",
        "\n",
        "PM_EVAL_QUESTIONS = [\n",
        "    {\n",
        "        \"id\": \"Q1\",\n",
        "        \"question\": \"Should we prioritize BMT-CRIT-042 and why?\",\n",
        "        \"intent\": \"End-to-end prioritization decision\",\n",
        "        \"expected_retrievers\": [\"semantic\", \"structural\", \"realtime\"],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q2\",\n",
        "        \"question\": \"Is BMT-CRIT-042 impacting high-value customers?\",\n",
        "        \"intent\": \"Customer impact and business risk\",\n",
        "        \"expected_retrievers\": [\"structural\", \"semantic\"],\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"Q3\",\n",
        "        \"question\": \"Is the BMT issue still happening right now?\",\n",
        "        \"intent\": \"Freshness-sensitive status check\",\n",
        "        \"expected_retrievers\": [\"realtime\"],\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "v9JNSvWLT_qM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 5B: RETRIEVER-LEVEL INSPECTION - Is each retriever doing what it was designed to do?\n",
        "#\n",
        "# Purpose:\n",
        "# - Validate each retriever independently\n",
        "# - Ensure semantic(FAISS), structural(NEO4J AURA), and real-time(MEMGRAPH) signals are distinct\n",
        "# - Catch silent failures early\n",
        "#\n",
        "# What to look for:\n",
        "# - FAISS -> narrative context, not ownership\n",
        "# - Neo4j -> facts, not prose\n",
        "# - Memgraph -> latest signal only\n",
        "#\n",
        "# PM value: This explains why Hybrid RAG exists\n",
        "# =============================================================\n",
        "\n",
        "def inspect_retrievers(question: str):\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(f\"QUESTION:\\n{question}\")\n",
        "\n",
        "    # --- Semantic RAG (FAISS) ---\n",
        "    print(\"\\n--- Semantic RAG (FAISS) ---\")\n",
        "    semantic_docs = faiss_retriever.invoke(question)\n",
        "    for i, d in enumerate(semantic_docs, 1):\n",
        "        print(f\"[{i}] {d.page_content[:200]}...\")\n",
        "\n",
        "    # --- Structural GraphRAG (Neo4j) ---\n",
        "    print(\"\\n--- Structural GraphRAG (Neo4j) ---\")\n",
        "    structural_docs = neo4j_retriever(question)\n",
        "    for d in structural_docs:\n",
        "        print(d.page_content)\n",
        "\n",
        "    # --- Real-Time GraphRAG (Memgraph) ---\n",
        "    print(\"\\n--- Real-Time GraphRAG (Memgraph) ---\")\n",
        "    realtime_docs = memgraph_retriever(question)\n",
        "    for d in realtime_docs:\n",
        "        print(d.page_content)\n",
        "\n",
        "\n",
        "# Run inspection for all top 3 questions\n",
        "for q in PM_EVAL_QUESTIONS:\n",
        "    inspect_retrievers(q[\"question\"])\n"
      ],
      "metadata": {
        "id": "gXGwzs3mkUni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 5C: HYBRID RAG ABLATION TESTING\n",
        "#\n",
        "# Question:\n",
        "# Does Hybrid RAG actually improve the PM decision?\n",
        "#\n",
        "# Method:\n",
        "# - Run the SAME question with increasing signal richness\n",
        "# - Compare recommendation, justification, and confidence\n",
        "#\n",
        "# Purpose:\n",
        "# - Prove the value of Hybrid RAG\n",
        "# - Surface decision deltas as retrievers are added\n",
        "#\n",
        "# Expected Insight:\n",
        "# - Semantic only â†’ verbose, weak authority\n",
        "# - Semantic + Graph â†’ precise, but incomplete\n",
        "# - Full Hybrid â†’ confident WITH tradeoffs\n",
        "#\n",
        "# PM Value:\n",
        "# This is the clearest evidence that architecture choices\n",
        "# materially affect decision quality.\n",
        "# =============================================================\n",
        "\n",
        "def run_ablation(question: str):\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(f\"QUESTION:\\n{question}\")\n",
        "\n",
        "    # Each ablation config intentionally controls\n",
        "    # which signals the synthesis LLM is allowed to see.\n",
        "    ablations = {\n",
        "        \"Semantic Only\": lambda q: format_context({\n",
        "            \"semantic\": faiss_retriever.invoke(q)\n",
        "        }),\n",
        "        \"Semantic + Structural\": lambda q: format_context({\n",
        "            \"semantic\": faiss_retriever.invoke(q),\n",
        "            # NOTE: neo4j_retriever is now hardened.\n",
        "            # Validation failures degrade confidence instead of crashing.\n",
        "            \"structural\": neo4j_retriever(q)\n",
        "        }),\n",
        "        \"Semantic + Real-Time\": lambda q: format_context({\n",
        "            \"semantic\": faiss_retriever.invoke(q),\n",
        "            \"realtime\": memgraph_retriever(q)\n",
        "        }),\n",
        "        \"Full Hybrid\": lambda q: format_context(\n",
        "            # RunnableParallel already uses hardened retrievers\n",
        "            parallel_retrieval.invoke(q)\n",
        "        ),\n",
        "    }\n",
        "\n",
        "    for name, ctx_fn in ablations.items():\n",
        "        print(f\"\\n--- {name} ---\")\n",
        "\n",
        "        try:\n",
        "            # Generate context for this ablation configuration\n",
        "            context = ctx_fn(question)\n",
        "\n",
        "            # Run synthesis ONLY on retrieved context\n",
        "            response = synthesis_llm.invoke(\n",
        "                FINAL_PROMPT.format(\n",
        "                    question=question,\n",
        "                    context=context\n",
        "                )\n",
        "            )\n",
        "\n",
        "            print(response.content)\n",
        "\n",
        "        except Exception as e:\n",
        "            # Ablation should never fail the notebook.\n",
        "            # Any failure here is an evaluation signal.\n",
        "            print(\n",
        "                \"[ABRATION ERROR]\\n\"\n",
        "                \"This configuration failed unexpectedly.\\n\"\n",
        "                f\"Error: {str(e)}\"\n",
        "            )\n",
        "\n",
        "\n",
        "# Run ablation on the primary PM prioritization question\n",
        "run_ablation(PM_EVAL_QUESTIONS[0][\"question\"])\n",
        "\n"
      ],
      "metadata": {
        "id": "7zocm_Dqks9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================================================\n",
        "# Stage 5D: DECISION QUALITY & CONFIDENCE REVIEW - Would a PM trust this output in a real roadmap meeting?\n",
        "# =============================================================\n",
        "# DECISION QUALITY RUBRIC\n",
        "#\n",
        "# Purpose:\n",
        "# - Evaluate decision usefulness, not language quality\n",
        "# - Penalize overconfidence more than uncertainty\n",
        "#\n",
        "# PM value: This aligns evals with real accountability, not synthetic benchmarks.\n",
        "# =============================================================\n",
        "\n",
        "DECISION_RUBRIC = {\n",
        "    \"Correctness\": \"Is the recommendation reasonable?\",\n",
        "    \"Justification\": \"Is the 'why' grounded in evidence?\",\n",
        "    \"Actionability\": \"Can this be defended to leadership?\",\n",
        "    \"Uncertainty Handling\": \"Are unknowns clearly stated?\",\n",
        "    \"Decision Confidence\": \"Does confidence match evidence strength?\",\n",
        "}\n",
        "\n",
        "def review_decision(question: str):\n",
        "    print(\"\\n\" + \"=\" * 100)\n",
        "    print(f\"QUESTION:\\n{question}\")\n",
        "\n",
        "    response = final_chain.invoke({\"question\": question})\n",
        "\n",
        "    print(\"\\n--- SYSTEM RESPONSE ---\\n\")\n",
        "    print(response)\n",
        "\n",
        "    print(\"\\n--- MANUAL SCORING (1â€“5) ---\")\n",
        "    for k, v in DECISION_RUBRIC.items():\n",
        "        print(f\"{k}: {v}\")\n",
        "\n",
        "\n",
        "# Review the primary decision question\n",
        "review_decision(PM_EVAL_QUESTIONS[0][\"question\"])\n"
      ],
      "metadata": {
        "id": "D7KEpYWgmR6n"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}